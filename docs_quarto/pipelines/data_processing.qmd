---
title: "Data Processing Pipeline"
---


This pipeline handles the preprocessing of review data for training a RoBERTa-based classification model. It includes data splitting and dataset preparation with MLflow tracking.

## Pipeline Structure

```{mermaid}
flowchart LR
    A[reviews] --> B[split_data_node]
    P[params:train_test_split] --> B
    B --> C[train_data]
    B --> D[test_data]
    C --> E[prepare_datasets_node]
    D --> E
    E --> F[train_dataset]
    E --> G[test_dataset]
    E --> H[tokenizer]
```

## Components

### Nodes

#### split_data_node

**Function**: `split_data`

**Description**: Splits input data into training and test sets while maintaining class distribution. Currently uses a subset of 5000 samples for testing purposes.

**Inputs**:
- `reviews`: Raw reviews DataFrame
- `params:train_test_split`: Parameters containing:
  - `test_size`: Fraction of data for testing
  - `random_state`: Random seed for reproducibility

**Outputs**:
- `train_data`: Training DataFrame
- `test_data`: Test DataFrame

**MLflow Tracking**:
- Logs raw data, training data, and test data as MLflow inputs

#### prepare_datasets_node

**Function**: `prepare_datasets`

**Description**: Creates PyTorch datasets from the split DataFrames using RoBERTa tokenizer.

**Inputs**:
- `train_data`: Training DataFrame
- `test_data`: Test DataFrame

**Outputs**:
- `train_dataset`: PyTorch training dataset
- `test_dataset`: PyTorch test dataset
- `tokenizer`: RoBERTa tokenizer instance

### Dataset Class

#### `ReviewDataset`

A PyTorch Dataset class for review data.

**Parameters**:
- `reviews`: List of review text
- `scores`: List of review scores
- `tokenizer`: Tokenizer
- `max_length`: Maximum length of input sequence (default: 512)

**Output Format**:
```python
{
    'input_ids': tensor([...]),        # Tokenized text
    'attention_mask': tensor([...]),   # Attention mask
    'labels': tensor(score)            # Zero-based score (0-4)
}
```