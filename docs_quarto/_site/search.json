[
  {
    "objectID": "api/server.html",
    "href": "api/server.html",
    "title": "API Documentation",
    "section": "",
    "text": "The Review Rating API provides endpoints for predicting ratings from review texts using our trained RoBERTa model. The API supports both single and batch predictions.\n\n\n\n\n\n\n\nflowchart TD\n    A[Client] --&gt; B[\"/predict\"]\n    A --&gt; C[\"/batch-predict\"]\n    A --&gt; D[\"/health\"]\n    A --&gt; E[\"/logs\"]\n    B --&gt; F[RoBERTa Model]\n    C --&gt; F\n    F --&gt; G[Rating Prediction]\n\n\n\n\n\n\n\n\n\n\n\nEndpoint: /predict\nMethod: POST\nMakes a rating prediction for a single review text.\nRequest Body:\n{\n    \"text\": \"string\"\n}\nResponse:\n{\n    \"rating\": \"int (1-5)\",\n    \"confidence\": \"float (0-1)\"\n}\n\n\n\nEndpoint: /batch-predict\nMethod: POST\nMakes rating predictions for multiple reviews in a single request.\nRequest Body:\n{\n    \"data\": [\n        {\n            \"text\": \"string\"\n        }\n    ]\n}\nResponse:\n{\n    \"predictions\": [\n        {\n            \"rating\": \"int (1-5)\",\n            \"confidence\": \"float (0-1)\"\n        }\n    ]\n}\n\n\n\nEndpoint: /health\nMethod: GET\nChecks if the API and model are operational.\nResponse:\n{\n    \"status\": \"string\",\n    \"model_loaded\": \"boolean\"\n}\n\n\n\nEndpoint: /logs\nMethod: GET\nRetrieves filtered API usage logs.\nQuery Parameters: - limit: Number of log entries to return (default: 100) - level: Log level filter (optional) - start_date: Start date filter (YYYY-MM-DD, optional) - end_date: End date filter (YYYY-MM-DD, optional)\nResponse: List of log entries with timestamp, logger, level, and message.\n\n\n\n\n\n\n\n\nclass InputData(BaseModel):\n    text: str\n\n\n\nclass BatchInputData(BaseModel):\n    data: List[InputData]\n\n\n\n\n\n\nclass PredictionResponse(BaseModel):\n    rating: int        # Rating from 1 to 5\n    confidence: float  # Confidence score between 0 and 1\n\n\n\nclass BatchPredictionResponse(BaseModel):\n    predictions: List[PredictionResponse]\n\n\n\n\n\nThe API returns appropriate HTTP status codes:\n\n200: Successful request\n422: Invalid input format\n500: Server error (e.g., model prediction failure)\n\nError responses include a detail message explaining the error.\n\n\n\nThe API allows cross-origin requests with the following configuration: - All origins allowed (\"*\") - All methods allowed - All headers allowed - Credentials supported\n\n\n\nThe API logs all requests and errors to model_usage.log with the following format:\ntimestamp - logger_name - log_level - message\n\n\n\nThe complete OpenAPI specification for this API can be found below:",
    "crumbs": [
      "API",
      "Server Documentation"
    ]
  },
  {
    "objectID": "api/server.html#overview",
    "href": "api/server.html#overview",
    "title": "API Documentation",
    "section": "",
    "text": "flowchart TD\n    A[Client] --&gt; B[\"/predict\"]\n    A --&gt; C[\"/batch-predict\"]\n    A --&gt; D[\"/health\"]\n    A --&gt; E[\"/logs\"]\n    B --&gt; F[RoBERTa Model]\n    C --&gt; F\n    F --&gt; G[Rating Prediction]",
    "crumbs": [
      "API",
      "Server Documentation"
    ]
  },
  {
    "objectID": "api/server.html#api-endpoints",
    "href": "api/server.html#api-endpoints",
    "title": "API Documentation",
    "section": "",
    "text": "Endpoint: /predict\nMethod: POST\nMakes a rating prediction for a single review text.\nRequest Body:\n{\n    \"text\": \"string\"\n}\nResponse:\n{\n    \"rating\": \"int (1-5)\",\n    \"confidence\": \"float (0-1)\"\n}\n\n\n\nEndpoint: /batch-predict\nMethod: POST\nMakes rating predictions for multiple reviews in a single request.\nRequest Body:\n{\n    \"data\": [\n        {\n            \"text\": \"string\"\n        }\n    ]\n}\nResponse:\n{\n    \"predictions\": [\n        {\n            \"rating\": \"int (1-5)\",\n            \"confidence\": \"float (0-1)\"\n        }\n    ]\n}\n\n\n\nEndpoint: /health\nMethod: GET\nChecks if the API and model are operational.\nResponse:\n{\n    \"status\": \"string\",\n    \"model_loaded\": \"boolean\"\n}\n\n\n\nEndpoint: /logs\nMethod: GET\nRetrieves filtered API usage logs.\nQuery Parameters: - limit: Number of log entries to return (default: 100) - level: Log level filter (optional) - start_date: Start date filter (YYYY-MM-DD, optional) - end_date: End date filter (YYYY-MM-DD, optional)\nResponse: List of log entries with timestamp, logger, level, and message.",
    "crumbs": [
      "API",
      "Server Documentation"
    ]
  },
  {
    "objectID": "api/server.html#data-models",
    "href": "api/server.html#data-models",
    "title": "API Documentation",
    "section": "",
    "text": "class InputData(BaseModel):\n    text: str\n\n\n\nclass BatchInputData(BaseModel):\n    data: List[InputData]\n\n\n\n\n\n\nclass PredictionResponse(BaseModel):\n    rating: int        # Rating from 1 to 5\n    confidence: float  # Confidence score between 0 and 1\n\n\n\nclass BatchPredictionResponse(BaseModel):\n    predictions: List[PredictionResponse]",
    "crumbs": [
      "API",
      "Server Documentation"
    ]
  },
  {
    "objectID": "api/server.html#error-handling",
    "href": "api/server.html#error-handling",
    "title": "API Documentation",
    "section": "",
    "text": "The API returns appropriate HTTP status codes:\n\n200: Successful request\n422: Invalid input format\n500: Server error (e.g., model prediction failure)\n\nError responses include a detail message explaining the error.",
    "crumbs": [
      "API",
      "Server Documentation"
    ]
  },
  {
    "objectID": "api/server.html#cors-configuration",
    "href": "api/server.html#cors-configuration",
    "title": "API Documentation",
    "section": "",
    "text": "The API allows cross-origin requests with the following configuration: - All origins allowed (\"*\") - All methods allowed - All headers allowed - Credentials supported",
    "crumbs": [
      "API",
      "Server Documentation"
    ]
  },
  {
    "objectID": "api/server.html#logging",
    "href": "api/server.html#logging",
    "title": "API Documentation",
    "section": "",
    "text": "The API logs all requests and errors to model_usage.log with the following format:\ntimestamp - logger_name - log_level - message",
    "crumbs": [
      "API",
      "Server Documentation"
    ]
  },
  {
    "objectID": "api/server.html#openapi-specification",
    "href": "api/server.html#openapi-specification",
    "title": "API Documentation",
    "section": "",
    "text": "The complete OpenAPI specification for this API can be found below:",
    "crumbs": [
      "API",
      "Server Documentation"
    ]
  },
  {
    "objectID": "pipelines/data_science.html",
    "href": "pipelines/data_science.html",
    "title": "Data Science Pipeline",
    "section": "",
    "text": "This pipeline handles the training and evaluation of the RoBERTa-based review rating model. It includes model training with MLflow tracking and comprehensive evaluation metrics.",
    "crumbs": [
      "Pipelines",
      "Data Science"
    ]
  },
  {
    "objectID": "pipelines/data_science.html#pipeline-structure",
    "href": "pipelines/data_science.html#pipeline-structure",
    "title": "Data Science Pipeline",
    "section": "Pipeline Structure",
    "text": "Pipeline Structure\n\n\n\n\n\nflowchart LR\n    A[train_dataset] --&gt; B[train_model_node]\n    D[params:model_params] --&gt; B\n    B --&gt; E[model]\n    E --&gt; F[evaluate_model_node]\n    C[test_dataset] --&gt; F\n    F --&gt; G[evaluation_report]",
    "crumbs": [
      "Pipelines",
      "Data Science"
    ]
  },
  {
    "objectID": "pipelines/data_science.html#components",
    "href": "pipelines/data_science.html#components",
    "title": "Data Science Pipeline",
    "section": "Components",
    "text": "Components\n\nNodes\n\ntrain_model_node\nFunction: train_model\nDescription: Trains a RoBERTa model for review classification using Hugging Face’s Transformers library.\nInputs: - train_dataset: Training PyTorch dataset - test_dataset: Test PyTorch dataset - params:model_params: Training parameters\nOutputs: - model: Trained PyTorch model\nMLflow Tracking: - Training dataset size - Class distribution - Training loss - Model artifacts\n\n\nevaluate_model_node\nFunction: evaluate_model\nDescription: Evaluates the trained model on the test dataset using batched prediction.\nInputs: - model: Trained model - test_dataset: Test PyTorch dataset\nOutputs: - evaluation_report: Classification report with performance metrics\nMLflow Tracking: - Detailed classification report - F1-scores for each class",
    "crumbs": [
      "Pipelines",
      "Data Science"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Review Rating Model Documentation",
    "section": "",
    "text": "Welcome to the documentation for the Review Rating Model project. This documentation provides comprehensive information about the project’s pipelines, API, and various documentation cards.\n\n\n\n\nThe project consists of two main pipelines:\n\nData Processing Pipeline\n\nHandles data splitting and preparation\nUses RoBERTa tokenizer for text processing\n\nData Science Pipeline\n\nTrains RoBERTa-based classification model\nPerforms model evaluation\n\n\n\n\n\nThe model is served through a FastAPI application that provides:\n\nReal-time prediction endpoints\nBatch prediction capabilities\nLogging and monitoring features\n\n\n\n\nThe project maintains three types of documentation cards:\n\nProject Card: Overall project information and metadata\nData Card: Dataset characteristics and quality metrics\nModel Card: Model specifications and performance metrics\n\n\n\n\n\nTo get started with the documentation:\n\nBrowse through the pipeline documentation to understand the data flow\nCheck the API documentation for integration details\nReview the cards for detailed specifications about different aspects of the project\n\n\nFramework: Kedro\nModel: RoBERTa (Hugging Face Transformers)\nExperiment Tracking: MLflow\nAPI: FastAPI\nDeployment: Docker\nDocumentation: Quarto"
  },
  {
    "objectID": "index.html#key-components",
    "href": "index.html#key-components",
    "title": "Review Rating Model Documentation",
    "section": "",
    "text": "The project consists of two main pipelines:\n\nData Processing Pipeline\n\nHandles data splitting and preparation\nUses RoBERTa tokenizer for text processing\n\nData Science Pipeline\n\nTrains RoBERTa-based classification model\nPerforms model evaluation\n\n\n\n\n\nThe model is served through a FastAPI application that provides:\n\nReal-time prediction endpoints\nBatch prediction capabilities\nLogging and monitoring features\n\n\n\n\nThe project maintains three types of documentation cards:\n\nProject Card: Overall project information and metadata\nData Card: Dataset characteristics and quality metrics\nModel Card: Model specifications and performance metrics"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Review Rating Model Documentation",
    "section": "",
    "text": "To get started with the documentation:\n\nBrowse through the pipeline documentation to understand the data flow\nCheck the API documentation for integration details\nReview the cards for detailed specifications about different aspects of the project\n\n\nFramework: Kedro\nModel: RoBERTa (Hugging Face Transformers)\nExperiment Tracking: MLflow\nAPI: FastAPI\nDeployment: Docker\nDocumentation: Quarto"
  },
  {
    "objectID": "cards/project_card.html",
    "href": "cards/project_card.html",
    "title": "Project Card",
    "section": "",
    "text": "Financial institutions face significant challenges in assessing credit risk for loan applications. Traditional credit scoring methods often miss nuanced patterns and can be biased against certain demographic groups. This leads to both missed opportunities for good borrowers being denied and increased risk from bad loans being approved. In today’s competitive banking environment, institutions need more sophisticated ways to evaluate credit applications while maintaining regulatory compliance and fairness.\n\n\n\nBanks need to accurately assess the creditworthiness of loan applicants to minimize default risk while maximizing approved loans to qualified borrowers. Currently, credit officers spend significant time manually reviewing applications and may make inconsistent decisions based on limited information. This results in both lost revenue from good applications being rejected and losses from bad loans being approved.\n\n\n\nPrimary customers are:\n\nCredit risk officers at medium to large retail banks\nLoan application processing teams\nBanking compliance officers responsible for fair lending practices\nFinancial institutions serving diverse demographic populations\n\n\n\n\n\nReduce loan default rates through more accurate risk assessment\nIncrease loan approval rates for qualified borrowers\nReduce application processing time\nImprove consistency and fairness in lending decisions\nEnable regulatory compliance through transparent decision-making\n\n\n\n\nCredit officers will receive a streamlined workflow where they: 1. Input standard loan application data into a familiar interface 2. Receive an immediate risk assessment score with key contributing factors 3. View detailed explanations of risk factors in an intuitive dashboard 4. Override recommendations with documented justification when needed 5. Generate standardized reports for audit and compliance purposes\n\n\n\n\nDeploy MVP risk assessment system to pilot branch by Q4 2024\nAchieve 90% user adoption among credit officers within 6 months of launch\nDemonstrate statistical fairness across demographic groups within the next quarter after launch\nObtain regulatory approval by mid 2025\n\n\n\n\n\nData Privacy & Security\n\nHandling sensitive financial and personal information\nEnsuring compliance with data protection regulations\nSecuring data transfer between systems\n\nUser Adoption\n\nResistance from experienced credit officers\nTraining requirements for new system\nIntegration with existing workflows\n\nRegulatory Compliance\n\nMeeting fair lending requirements\nProviding required transparency in decisions\nMaintaining audit trails",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#background",
    "href": "cards/project_card.html#background",
    "title": "Project Card",
    "section": "",
    "text": "Financial institutions face significant challenges in assessing credit risk for loan applications. Traditional credit scoring methods often miss nuanced patterns and can be biased against certain demographic groups. This leads to both missed opportunities for good borrowers being denied and increased risk from bad loans being approved. In today’s competitive banking environment, institutions need more sophisticated ways to evaluate credit applications while maintaining regulatory compliance and fairness.",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#problem",
    "href": "cards/project_card.html#problem",
    "title": "Project Card",
    "section": "",
    "text": "Banks need to accurately assess the creditworthiness of loan applicants to minimize default risk while maximizing approved loans to qualified borrowers. Currently, credit officers spend significant time manually reviewing applications and may make inconsistent decisions based on limited information. This results in both lost revenue from good applications being rejected and losses from bad loans being approved.",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#customer",
    "href": "cards/project_card.html#customer",
    "title": "Project Card",
    "section": "",
    "text": "Primary customers are:\n\nCredit risk officers at medium to large retail banks\nLoan application processing teams\nBanking compliance officers responsible for fair lending practices\nFinancial institutions serving diverse demographic populations",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#value-proposition",
    "href": "cards/project_card.html#value-proposition",
    "title": "Project Card",
    "section": "",
    "text": "Reduce loan default rates through more accurate risk assessment\nIncrease loan approval rates for qualified borrowers\nReduce application processing time\nImprove consistency and fairness in lending decisions\nEnable regulatory compliance through transparent decision-making",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#product",
    "href": "cards/project_card.html#product",
    "title": "Project Card",
    "section": "",
    "text": "Credit officers will receive a streamlined workflow where they: 1. Input standard loan application data into a familiar interface 2. Receive an immediate risk assessment score with key contributing factors 3. View detailed explanations of risk factors in an intuitive dashboard 4. Override recommendations with documented justification when needed 5. Generate standardized reports for audit and compliance purposes",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#objectives",
    "href": "cards/project_card.html#objectives",
    "title": "Project Card",
    "section": "",
    "text": "Deploy MVP risk assessment system to pilot branch by Q4 2024\nAchieve 90% user adoption among credit officers within 6 months of launch\nDemonstrate statistical fairness across demographic groups within the next quarter after launch\nObtain regulatory approval by mid 2025",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#risks-challenges",
    "href": "cards/project_card.html#risks-challenges",
    "title": "Project Card",
    "section": "",
    "text": "Data Privacy & Security\n\nHandling sensitive financial and personal information\nEnsuring compliance with data protection regulations\nSecuring data transfer between systems\n\nUser Adoption\n\nResistance from experienced credit officers\nTraining requirements for new system\nIntegration with existing workflows\n\nRegulatory Compliance\n\nMeeting fair lending requirements\nProviding required transparency in decisions\nMaintaining audit trails",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#task",
    "href": "cards/project_card.html#task",
    "title": "Project Card",
    "section": "Task",
    "text": "Task\nThis is a binary classification problem predicting credit risk based on the provided dataset (dataset_id_96.csv). According to the code and data samples, we need to predict ‘y’ (True/False) indicating whether a loan application represents a good or bad credit risk.",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#metrics",
    "href": "cards/project_card.html#metrics",
    "title": "Project Card",
    "section": "Metrics",
    "text": "Metrics\n\nPrimary: F1 score (currently being tracked in evaluate_model function)\nSupporting metrics:\n\nAccuracy\nPrecision\nRecall\nTarget drift score (monitored via Evidently)",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#evaluation",
    "href": "cards/project_card.html#evaluation",
    "title": "Project Card",
    "section": "Evaluation",
    "text": "Evaluation\nBased on the provided pipeline code, evaluation happens through: 1. Train/test split with configurable test size 2. Target drift detection between training and test sets using Evidently 3. Model performance tracking via MLflow 4. Production monitoring through FastAPI logging 5. Regular retraining evaluation",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#data",
    "href": "cards/project_card.html#data",
    "title": "Project Card",
    "section": "Data",
    "text": "Data\n\nPrimary dataset: dataset_id_96.csv\nFeatures include:\n\nCredit history attributes (checking_status, credit_history, etc.)\nPersonal information (age, employment, etc.)\nLoan details (duration, credit_amount, etc.)\nGenerated features (X_1 through X_10)\n\nData Pipeline:\n\nRaw data ingestion via Kedro\nData quality checks\nPreprocessing including scaling and encoding\nFeature engineering\nTrain/test splitting",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#planroadmap",
    "href": "cards/project_card.html#planroadmap",
    "title": "Project Card",
    "section": "Plan/Roadmap",
    "text": "Plan/Roadmap\n\nPhase 1 - Initial Development\n\nEnhance current data pipeline\nImplement additional data quality checks\nDevelop model monitoring dashboard\nComplete initial model training\n\nPhase 2 - Pilot\n\nDeploy to pilot branch\nGather user feedback\nRefine model based on real usage\nImplement A/B testing framework\n\nPhase 3 - Scale\n\nRoll out to additional branches\nImplement automated retraining pipeline\nEnhance monitoring and alerting\nDevelop fallback procedures",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#continuous-improvement",
    "href": "cards/project_card.html#continuous-improvement",
    "title": "Project Card",
    "section": "Continuous Improvement",
    "text": "Continuous Improvement\n\nAutomated monitoring via:\n\nMLflow experiment tracking\nFastAPI request logging\nEvidently drift detection\n\nRegular Updates:\n\nModel retraining based on drift detection\nFeature importance analysis\nPerformance metric tracking\nUser feedback incorporation",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/project_card.html#resources",
    "href": "cards/project_card.html#resources",
    "title": "Project Card",
    "section": "Resources",
    "text": "Resources\n\nHuman Resources\n\nData Science Team:\n\n1 ML Engineers (model development)\n1 Data Engineer (pipeline maintenance)\n1 MLOps Engineer (deployment/monitoring)\n\nProduct Team:\n\n1 Product Manager\n1 UX Designer\n1 Full-stack Developers\n\nDomain Experts:\n\n1 Credit Risk Officer\n1 Compliance Officer\n\n\n\n\nCompute Resources\n\nDevelopment:\n\nKedro pipeline execution environment\nMLflow tracking server\nDevelopment databases\n\nProduction:\n\nFastAPI server for model serving\nModel artifact storage\nMonitoring infrastructure\nLoad balancing for high availability\nBackup and disaster recovery systems\n\nStorage:\n\nSecure data warehouse for sensitive information\nModel registry\nLog storage\nBackup storage",
    "crumbs": [
      "Cards",
      "Project Card"
    ]
  },
  {
    "objectID": "cards/model_card.html",
    "href": "cards/model_card.html",
    "title": "Model Card",
    "section": "",
    "text": "This model card provides information about the Review Rating Prediction model, which predicts product review ratings on a scale of 1-5 stars based on review text.\n\n\nCode\nimport os\nimport mlflow\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport json\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n\n# Set MLflow tracking URI\nroot_dir = os.path.dirname(os.path.dirname(os.getcwd()))\nmlflow.set_tracking_uri(os.path.join(root_dir, 'mlruns'))\n\n# Helper functions for MLflow dataset handling\ndef get_dataset_by_context(inputs, context):\n    \"\"\"Get dataset information by context tag\"\"\"\n    for input_data in inputs:\n        for tag in input_data.tags:\n            if tag.key == 'mlflow.data.context' and tag.value == context:\n                return input_data.dataset\n    return None\n\ndef get_schema_info(dataset):\n    \"\"\"Extract schema information from dataset\"\"\"\n    if dataset and dataset.schema:\n        schema_dict = json.loads(dataset.schema)\n        return schema_dict.get('mlflow_colspec', [])\n    return []\n\ndef get_row_count(dataset):\n    \"\"\"Get number of rows from dataset profile\"\"\"\n    if dataset and dataset.profile:\n        profile_dict = json.loads(dataset.profile)\n        return profile_dict.get('num_rows', 0)\n    return 0\n\n# Get model and run information\nMODEL_NAME = \"review_rating_model\"\nclient = mlflow.tracking.MlflowClient()\n# latest_version = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])[0]\nlatest_version = client.get_model_version(\"review_rating_model\", 9)\nrun = mlflow.get_run(latest_version.run_id)\n\n\n# Get dataset information\nvalidation_data = get_dataset_by_context(run.inputs.dataset_inputs, 'validation')\ntraining_data = get_dataset_by_context(run.inputs.dataset_inputs, 'training')\nraw_data = get_dataset_by_context(run.inputs.dataset_inputs, 'raw_data')\n\n# Display basic model info\nprint(f\"Model Version: {latest_version.version}\")\nprint(f\"Last Updated: {datetime.fromtimestamp(run.info.start_time/1000).strftime('%Y-%m-%d %H:%M:%S')}\")\n\n\nModel Version: 9\nLast Updated: 2024-11-21 00:58:51",
    "crumbs": [
      "Cards",
      "Model Card"
    ]
  },
  {
    "objectID": "cards/model_card.html#model-description",
    "href": "cards/model_card.html#model-description",
    "title": "Model Card",
    "section": "",
    "text": "This model card provides information about the Review Rating Prediction model, which predicts product review ratings on a scale of 1-5 stars based on review text.\n\n\nCode\nimport os\nimport mlflow\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport json\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n\n# Set MLflow tracking URI\nroot_dir = os.path.dirname(os.path.dirname(os.getcwd()))\nmlflow.set_tracking_uri(os.path.join(root_dir, 'mlruns'))\n\n# Helper functions for MLflow dataset handling\ndef get_dataset_by_context(inputs, context):\n    \"\"\"Get dataset information by context tag\"\"\"\n    for input_data in inputs:\n        for tag in input_data.tags:\n            if tag.key == 'mlflow.data.context' and tag.value == context:\n                return input_data.dataset\n    return None\n\ndef get_schema_info(dataset):\n    \"\"\"Extract schema information from dataset\"\"\"\n    if dataset and dataset.schema:\n        schema_dict = json.loads(dataset.schema)\n        return schema_dict.get('mlflow_colspec', [])\n    return []\n\ndef get_row_count(dataset):\n    \"\"\"Get number of rows from dataset profile\"\"\"\n    if dataset and dataset.profile:\n        profile_dict = json.loads(dataset.profile)\n        return profile_dict.get('num_rows', 0)\n    return 0\n\n# Get model and run information\nMODEL_NAME = \"review_rating_model\"\nclient = mlflow.tracking.MlflowClient()\n# latest_version = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])[0]\nlatest_version = client.get_model_version(\"review_rating_model\", 9)\nrun = mlflow.get_run(latest_version.run_id)\n\n\n# Get dataset information\nvalidation_data = get_dataset_by_context(run.inputs.dataset_inputs, 'validation')\ntraining_data = get_dataset_by_context(run.inputs.dataset_inputs, 'training')\nraw_data = get_dataset_by_context(run.inputs.dataset_inputs, 'raw_data')\n\n# Display basic model info\nprint(f\"Model Version: {latest_version.version}\")\nprint(f\"Last Updated: {datetime.fromtimestamp(run.info.start_time/1000).strftime('%Y-%m-%d %H:%M:%S')}\")\n\n\nModel Version: 9\nLast Updated: 2024-11-21 00:58:51",
    "crumbs": [
      "Cards",
      "Model Card"
    ]
  },
  {
    "objectID": "cards/model_card.html#model-architecture",
    "href": "cards/model_card.html#model-architecture",
    "title": "Model Card",
    "section": "Model Architecture",
    "text": "Model Architecture\n\nType: RoBERTa-based Sequence Classification\nBase Model: RoBERTa Base\nTask: 5-class classification for review rating prediction\nOutput: Rating prediction (1-5 stars)",
    "crumbs": [
      "Cards",
      "Model Card"
    ]
  },
  {
    "objectID": "cards/model_card.html#dataset-overview",
    "href": "cards/model_card.html#dataset-overview",
    "title": "Model Card",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\n\nCode\n# Display dataset statistics\nprint(\"Dataset Statistics:\")\n# print(f\"Training samples: {get_row_count(training_data):,}\")\n# print(f\"Validation samples: {get_row_count(validation_data):,}\")\n# print(f\"Raw data samples: {get_row_count(raw_data):,}\")\n\nprint(f\"Training samples: {454764:,}\")\nprint(f\"Validation samples: {113690:,}\")\nprint(f\"Raw data samples: {568454:,}\")\n\n# Display schema information\nif raw_data:\n    print(\"\\nFeature Information:\")\n    for col in get_schema_info(raw_data):\n        print(f\"- {col['name']} ({col['type']})\")\n\n\nDataset Statistics:\nTraining samples: 454,764\nValidation samples: 113,690\nRaw data samples: 568,454",
    "crumbs": [
      "Cards",
      "Model Card"
    ]
  },
  {
    "objectID": "cards/model_card.html#model-performance",
    "href": "cards/model_card.html#model-performance",
    "title": "Model Card",
    "section": "Model Performance",
    "text": "Model Performance\n\n\nCode\n# Get histories for loss metrics\nmetrics_to_plot = ['train_loss', 'eval_loss', 'learning_rate', 'grad_norm']\nhistories = {\n    metric: client.get_metric_history(run.info.run_id, metric)\n    for metric in metrics_to_plot\n}\n\n# Create two subplots\nfig = go.Figure()\n\n# Add traces for losses\nfinal_metrics = run.data.metrics\n\n# Create evaluation loss plot\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=steps,\n    y=values,\n    name='Evaluation Loss',\n    mode='lines+markers'\n))\n\nfig.update_layout(\n    title='Evaluation Loss During Training',\n    xaxis_title='Step',\n    yaxis_title='Loss',\n    hovermode='x unified'\n)\nfig.show()\n\n# Display final metrics\nfinal_metrics = run.data.metrics\nprint(\"\\nFinal Metrics:\")\nprint(f\"Training Loss: {final_metrics.get('train_loss', 'N/A'):.4f}\")\nprint(f\"Evaluation Loss: {final_metrics.get('eval_loss', 'N/A'):.4f}\")\nprint(f\"Training Runtime: {final_metrics.get('train_runtime', 'N/A'):.2f}s\")\nprint(f\"Samples/second: {final_metrics.get('train_samples_per_second', 'N/A'):.2f}\")\n\n\n                                                \n\n\n\nFinal Metrics:\nTraining Loss: 0.6280\nEvaluation Loss: 0.5381\nTraining Runtime: 23582.44s\nSamples/second: 3.39",
    "crumbs": [
      "Cards",
      "Model Card"
    ]
  },
  {
    "objectID": "cards/model_card.html#training-configuration",
    "href": "cards/model_card.html#training-configuration",
    "title": "Model Card",
    "section": "Training Configuration",
    "text": "Training Configuration\n\n\nCode\nparams = run.data.params\nfinal_metrics = run.data.metrics\n\ntraining_params = {\n    'Epochs': params.get('num_train_epochs', 'N/A'),\n    'Train Batch Size': params.get('per_device_train_batch_size', 'N/A'),\n    'Eval Batch Size': params.get('per_device_eval_batch_size', 'N/A'),\n    'Learning Rate': params.get('learning_rate', 'N/A'),\n    'Weight Decay': params.get('weight_decay', 'N/A'),\n    'Warmup Ratio': params.get('warmup_ratio', 'N/A'),\n    'Gradient Accumulation': params.get('gradient_accumulation_steps', 'N/A'),\n    'Max Gradient Norm': params.get('max_grad_norm', 'N/A'),\n    'Max Steps': params.get('max_steps', 'N/A'),\n    'FP16': params.get('fp16', 'N/A'),\n    'Eval Strategy': params.get('evaluation_strategy', 'N/A'),\n    'Eval Steps': params.get('eval_steps', 'N/A'),\n    'Save Steps': params.get('save_steps', 'N/A'),\n    'Dataloader Workers': params.get('dataloader_num_workers', 'N/A')\n}\n\nprint(\"Training Configuration:\")\nfor param, value in training_params.items():\n    print(f\"{param}: {value}\")\n\n\nTraining Configuration:\nEpochs: 1\nTrain Batch Size: 16\nEval Batch Size: 16\nLearning Rate: 2e-05\nWeight Decay: 0.01\nWarmup Ratio: 0.1\nGradient Accumulation: 2\nMax Gradient Norm: 1.0\nMax Steps: 2500\nFP16: True\nEval Strategy: steps\nEval Steps: 100\nSave Steps: 100\nDataloader Workers: 2",
    "crumbs": [
      "Cards",
      "Model Card"
    ]
  },
  {
    "objectID": "cards/model_card.html#model-usage-guidelines",
    "href": "cards/model_card.html#model-usage-guidelines",
    "title": "Model Card",
    "section": "Model Usage Guidelines",
    "text": "Model Usage Guidelines\n\nIntended Uses\n\nAutomated prediction of product review ratings\nBulk processing of historical reviews\nQuality control for review submissions\n\n\n\nOut-of-Scope Uses\n\nSentiment analysis of non-product-related text\nAnalysis of non-English reviews\nReal-time/streaming predictions",
    "crumbs": [
      "Cards",
      "Model Card"
    ]
  },
  {
    "objectID": "cards/model_card.html#limitations-and-biases",
    "href": "cards/model_card.html#limitations-and-biases",
    "title": "Model Card",
    "section": "Limitations and Biases",
    "text": "Limitations and Biases\n\nTechnical Limitations\n\nMaximum sequence length: 512 tokens\nEnglish-language only\nLimited to product review domain\nBatch processing only (not optimized for real-time)\n\n\n\nKnown Biases\n\nTraining data class imbalance (see distribution plot)\nDomain-specific vocabulary\nMay perform differently across product categories",
    "crumbs": [
      "Cards",
      "Model Card"
    ]
  },
  {
    "objectID": "cards/model_card.html#model-maintenance",
    "href": "cards/model_card.html#model-maintenance",
    "title": "Model Card",
    "section": "Model Maintenance",
    "text": "Model Maintenance\n\nMonitoring Requirements\n\nData drift in review patterns\nPerformance across rating classes\nClass distribution changes\nCoverage of product categories\n\n\n\nRetraining Triggers\n\nPerformance below 0.8 F1-score per class\nSignificant data drift detected\nMajor changes in review patterns\n6-month deployment period",
    "crumbs": [
      "Cards",
      "Model Card"
    ]
  },
  {
    "objectID": "cards/model_card.html#model-artifacts-and-governance",
    "href": "cards/model_card.html#model-artifacts-and-governance",
    "title": "Model Card",
    "section": "Model Artifacts and Governance",
    "text": "Model Artifacts and Governance\n\n\nCode\nprint(\"Model Information:\")\nprint(f\"Model Location: {latest_version.source}\")\nprint(f\"Run ID: {run.info.run_id}\")\nprint(f\"Artifact URI: {run.info.artifact_uri}\")\n\n\nModel Information:\nModel Location: file:///home/tathagat/workspace/projects/MLPE/tathagata-ai-839/review-rating/mlruns/863057453145536184/27e869ed62a3496fa284da0d956ee9e6/artifacts/model\nRun ID: 27e869ed62a3496fa284da0d956ee9e6\nArtifact URI: file:///home/tathagat/workspace/projects/MLPE/tathagata-ai-839/review-rating/mlruns/863057453145536184/27e869ed62a3496fa284da0d956ee9e6/artifacts",
    "crumbs": [
      "Cards",
      "Model Card"
    ]
  },
  {
    "objectID": "cards/data_card.html",
    "href": "cards/data_card.html",
    "title": "Data Card",
    "section": "",
    "text": "Code\nimport os\nimport pandas as pd\nimport json\nfrom evidently.metric_preset import DataQualityPreset\nfrom evidently.metrics import DatasetCorrelationsMetric\nfrom evidently.report import Report\nimport json\nfrom rich import print\nfrom pathlib import Path\n\nfrom pathlib import Path\n\n# Data Quality Functions\ndef generate_data_quality_report(data: pd.DataFrame) -&gt; None:\n    data_quality_report = Report(\n        metrics=[DataQualityPreset(), DatasetCorrelationsMetric()]\n    )\n    relevant_data = data[['HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Id']]\n    data_quality_report.run(current_data=relevant_data, reference_data=None)\n    root_dir = os.getcwd()\n    report_path = os.path.join(root_dir, 'docs_quarto', 'data_quality', 'data_quality_report.qmd')\n    os.makedirs(os.path.dirname(report_path), exist_ok=True)\n    html_content = data_quality_report.get_html()\n    with open(report_path, 'w') as f:\n        f.write(\"```{=html}\")\n        f.write(html_content)\n        f.write(\"```\")\n\ndef get_data_quality_metrics(data: pd.DataFrame) -&gt; dict:\n    metrics = {\n        \"num_features\": len(data.columns),\n        \"num_rows\": len(data),\n        \"missing_values\": data.isnull().sum().sum() / (data.shape[0] * data.shape[1])\n    }\n    return metrics\n\ndef run_data_quality_checks(df: pd.DataFrame) -&gt; dict:\n    generate_data_quality_report(df)\n    metrics = get_data_quality_metrics(df)\n    return metrics\n\n# Node Function\ndef create_data_card(loaded_data: pd.DataFrame, data_quality_metrics: dict) -&gt; str:\n    data_card = {\n        \"dataset_name\": \"Amazon Fine Food Reviews\",\n        \"number_of_rows\": len(loaded_data),\n        \"number_of_features\": len(loaded_data.columns),\n        \"feature_names\": list(loaded_data.columns),\n        \"data_quality_metrics\": data_quality_metrics,\n    }\n    return json.dumps(data_card)\n\n# Example usage\ndata_path = r'data/01_raw/Reviews.csv'  # Change this to the path of your dataset\ndata = pd.read_csv(data_path)\ndata_quality_metrics = run_data_quality_checks(data)\ndata_card_json = create_data_card(data, data_quality_metrics)\ndata_card = json.loads(data_card_json)\n\n\n\nName: Amazon Fine Food Reviews\nDescription: This dataset contains over 500,000 food reviews from Amazon users up to October 2012. Reviews include information about the product and user, with ratings, helpfulness votes, and a summary.\nDataset from time: Oct 1999 - Oct 2012\nVersion: 2.0\n\n\nDataset Characteristics\n\n\nCode\n# project_dir = Path().absolute().parent\n# data_card_path = project_dir / \"data\" / \"08_reporting\" / \"data-card\" / \"data_card.json\"\n\n# with open(data_card_path) as f:\n#     data_card = json.load(f)\n#     data_card = json.loads(data_card)\nprint(f\"\"\"• [bold]Number of Instances[/bold]: {data_card['number_of_rows']}\n• [bold]Number of Features[/bold]: {data_card['number_of_features']}\n• [bold]Target Variable[/bold]: y (boolean)\"\"\")\n\n\n• Number of Instances: 568454\n• Number of Features: 10\n• Target Variable: y (boolean)\n\n\n\n\n\nFeatures\n\n\nCode\nfeatures_list = [f\"{i + 1}. {data_card['feature_names'][i]}\" \n                 for i in range(len(data_card[\"feature_names\"]))]\nprint(\"\\n\".join(features_list))\n\n\n1. Id\n2. ProductId\n3. UserId\n4. ProfileName\n5. HelpfulnessNumerator\n6. HelpfulnessDenominator\n7. Score\n8. Time\n9. Summary\n10. Text\n\n\n\n\n\nData Collection\n\nMethod: Unknown\n\n\n\nIntended Use\n\nSentiment Analysis: Analyzing the sentiment of the reviews—whether they are positive, negative, or neutral. This is useful for understanding consumer opinions and improving products or services.\nText Classification: Classifying reviews into various categories based on their content, which could help in automatically sorting feedback into different areas such as packaging, taste, and customer service.\nRecommendation Systems: Using the ratings and reviews to build recommendation systems that can suggest products to users based on the preferences of similar users.\nLanguage Modeling: Training language models to generate text that mimics user-generated content, which can be useful for creating automated responses or new reviews for training.\nFeature Extraction: Extracting and analyzing specific features from the text, such as the use of adjectives, to study how language use affects the perception of a product.\nData Visualization: Visualizing data to identify trends and patterns in consumer behavior over time, across different products, or among different groups of reviewers.\n\n\n\nEthical Considerations\n\nEnsure fair and unbiased use of the data, particularly regarding protected attributes like personal status.\nBe cautious of potential biases in the original data collection process.\nConsider the implications of using this data for decision-making in financial contexts.\n\n\n\nKnown Limitations\n\nBias in Reviews: The dataset may not represent the general population as it only includes reviews from Amazon users who are more likely to provide feedback. Users who write reviews might differ significantly in their tastes and expectations from the average consumer.\nRating Inflation: There is often a tendency for review datasets to show rating inflation where the number of high ratings disproportionately exceeds lower ratings. This can skew analysis, particularly if the goal is to understand negative feedback.\nOutdated Information: As products and consumer preferences evolve over time, the reviews, especially older ones, may not accurately reflect the current status or quality of a product.\nMissing Context: Reviews may reference specific product versions, experiences, or events not fully detailed in the text, leading to potential misinterpretation of the sentiment or content of the review.\nText Quality: The quality of text in reviews can vary greatly. Some reviews may be very brief or poorly written, offering little useful information for analysis, while others may be verbose and detailed.\nSpam and Fake Reviews: The presence of spam or fake reviews can distort analysis, leading to inaccurate conclusions unless methods are in place to identify and filter out such content.\nLimited Demographic Data: The dataset primarily focuses on the text and ratings of reviews without providing detailed demographic information about the reviewers, which could be crucial for understanding preferences across different user segments.",
    "crumbs": [
      "Cards",
      "Data Card"
    ]
  },
  {
    "objectID": "pipelines/data_processing.html",
    "href": "pipelines/data_processing.html",
    "title": "Data Processing Pipeline",
    "section": "",
    "text": "This pipeline handles the preprocessing of review data for training a RoBERTa-based classification model. It includes data splitting and dataset preparation with MLflow tracking.",
    "crumbs": [
      "Pipelines",
      "Data Processing"
    ]
  },
  {
    "objectID": "pipelines/data_processing.html#pipeline-structure",
    "href": "pipelines/data_processing.html#pipeline-structure",
    "title": "Data Processing Pipeline",
    "section": "Pipeline Structure",
    "text": "Pipeline Structure\n\n\n\n\n\nflowchart LR\n    A[reviews] --&gt; B[split_data_node]\n    P[params:train_test_split] --&gt; B\n    B --&gt; C[train_data]\n    B --&gt; D[test_data]\n    C --&gt; E[prepare_datasets_node]\n    D --&gt; E\n    E --&gt; F[train_dataset]\n    E --&gt; G[test_dataset]\n    E --&gt; H[tokenizer]",
    "crumbs": [
      "Pipelines",
      "Data Processing"
    ]
  },
  {
    "objectID": "pipelines/data_processing.html#components",
    "href": "pipelines/data_processing.html#components",
    "title": "Data Processing Pipeline",
    "section": "Components",
    "text": "Components\n\nNodes\n\nsplit_data_node\nFunction: split_data\nDescription: Splits input data into training and test sets while maintaining class distribution. Currently uses a subset of 5000 samples for testing purposes.\nInputs: - reviews: Raw reviews DataFrame - params:train_test_split: Parameters containing: - test_size: Fraction of data for testing - random_state: Random seed for reproducibility\nOutputs: - train_data: Training DataFrame - test_data: Test DataFrame\nMLflow Tracking: - Logs raw data, training data, and test data as MLflow inputs\n\n\nprepare_datasets_node\nFunction: prepare_datasets\nDescription: Creates PyTorch datasets from the split DataFrames using RoBERTa tokenizer.\nInputs: - train_data: Training DataFrame - test_data: Test DataFrame\nOutputs: - train_dataset: PyTorch training dataset - test_dataset: PyTorch test dataset - tokenizer: RoBERTa tokenizer instance\n\n\n\nDataset Class\n\nReviewDataset\nA PyTorch Dataset class for review data.\nParameters: - reviews: List of review text - scores: List of review scores - tokenizer: Tokenizer - max_length: Maximum length of input sequence (default: 512)\nOutput Format:\n{\n    'input_ids': tensor([...]),        # Tokenized text\n    'attention_mask': tensor([...]),   # Attention mask\n    'labels': tensor(score)            # Zero-based score (0-4)\n}",
    "crumbs": [
      "Pipelines",
      "Data Processing"
    ]
  },
  {
    "objectID": "pipelines/data_monitoring.html",
    "href": "pipelines/data_monitoring.html",
    "title": "Data Monitoring Pipeline",
    "section": "",
    "text": "This pipeline implements data validation and drift detection for the review classification system. It uses the Evidently framework to monitor data quality and detect potential data distribution changes.",
    "crumbs": [
      "Pipelines",
      "Data Monitoring"
    ]
  },
  {
    "objectID": "pipelines/data_monitoring.html#pipeline-structure",
    "href": "pipelines/data_monitoring.html#pipeline-structure",
    "title": "Data Monitoring Pipeline",
    "section": "Pipeline Structure",
    "text": "Pipeline Structure\n\n\n\n\n\nflowchart LR\n    A[reference_data] --&gt; B[run_data_validation_node]\n    C[current_data] --&gt; B\n    P[params:monitoring] --&gt; B\n    B --&gt; D[test_results]\n    B --&gt; E[html_report]",
    "crumbs": [
      "Pipelines",
      "Data Monitoring"
    ]
  },
  {
    "objectID": "pipelines/data_monitoring.html#components",
    "href": "pipelines/data_monitoring.html#components",
    "title": "Data Monitoring Pipeline",
    "section": "Components",
    "text": "Components\n\nNodes\n\nrun_data_validation_node\nFunction: run_data_validation\nDescription: Runs a comprehensive suite of data validation tests comparing reference data with current data. Generates both programmatic test results and a human-readable HTML report.\nInputs: - reference_data: Reference DataFrame (baseline) - current_data: Current DataFrame to validate - params:monitoring: Parameters containing: - drift: Drift detection settings - cat_stattest: Statistical test for categorical variables - cat_stattest_threshold: P-value threshold for drift detection - drift_share: Maximum allowed share of drifted features\nOutputs: - test_results: Dictionary containing detailed test results - HTML report saved to data/09_monitoring/test_results.html\nTest Suite Components:\n\nMissing Values Test\n\nMonitors the share of missing values in the “Text” column\nFlags significant changes in missing value patterns\n\nData Structure Tests\n\nTestNumberOfConstantColumns: Detects columns with constant values\nTestNumberOfDuplicatedColumns: Identifies duplicate columns\nTestNumberOfDuplicatedRows: Monitors for duplicate records\n\nData Drift Detection\n\nMonitors distribution changes in the “Score” column\nUses statistical testing with configurable thresholds\nReports both individual feature drift and overall dataset drift\n\n\n\n\n\nError Handling and Logging\nThe pipeline implements comprehensive error handling and logging:\n\nDetailed Test Results: Each test’s status, parameters, and description are logged\nContext-Aware Failures: Failed tests include specific recommendations for remediation\nHTML Reports: Generated for each validation run with visualizations\nStructured Error Messages: Failures include:\n\nTest name and status\nTest parameters and thresholds\nDescriptive context\nRecommended actions\n\n\n\n\nUsage Example\n# Pipeline parameters example\nmonitoring_params = {\n    \"drift\": {\n        \"cat_stattest\": \"chi2\",\n        \"cat_stattest_threshold\": 0.05,\n        \"drift_share\": 0.3\n    }\n}\n\n# Running the pipeline\ntest_results = run_data_validation(\n    reference_data=reference_df,\n    current_data=current_df,\n    monitoring_params=monitoring_params\n)\n\n\nFailure Scenarios\nThe pipeline will raise a ValueError with detailed context when: - Data quality tests fail (e.g., unexpected duplicates) - Column structure changes are detected - Significant data drift is identified beyond thresholds",
    "crumbs": [
      "Pipelines",
      "Data Monitoring"
    ]
  }
]